---
title: "Resource Selection Function Guide"
author: "Jessica Bonin"
date: "2023-04-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Resource Selection Function Guide {.tabset .tabset-fade .tabset-pills}

This GitHub page was created to allow the reader to follow along with the coding process of running an RSF.

## Cleaning {.tabset}

Cleaning the data is an important first step to any analysis.

### Initial Cleaning

The data we are using comes from GPS collars. This means there are rows within the raw data that are not usable. This can be due to the collar not collecting a location or collecting an inaccurate location. It is important to remove all rows with no data and with inaccurate data (determined here by the dilution measurement). In addition, the time the bears were in the den are removed. This will ensure that we are not extracting habitat use from a time when the bears are not actively changing resource use. We also wanted to convert the time of the GPS fixes from UTC to standard time. This will be more consistent with GIS layers. Seasons were decided on general habitat condition changes like food availability. Seasons were then added to the data in order to conduct seasonal analysis later on. Once the data are cleaned, it is important to have uniform structure to be able to use the data later on.


**Set Paths**
Setting paths is important to make sure you are working with data in the correct file folder. 
It will vary based on where your personal data is stored
```{r}
path.root <- "C:/Users/jessi/University of Massachusetts/Jessica-BearProject - General/Final Data"
path.data <- paste(path.root, "/Raw Data", sep = "")
path.info <- paste(path.root, "/Bear Information", sep = "")
```

**Libraries**
Installing and using the right packages is important to ensure your functions run properly.
```{r}
library(lubridate)
```

**List All Bear Files**
This allows you to pull in all of the file names from one file folder. It is important to note that this is not the data itself, just the names of the files in the folder.
```{r}
setwd(path.data)
# Pulling in by the pattern lists every file in the folder with that syntax
# In this case we are using the file extension 
file.list <- list.files(pattern = "\\.csv$", full.names = FALSE)
file.list
```

**Creating the Loop**
Doing this in a loop allows R to run the code for each file one after the other.
```{r}
for (i in 1:length(file.list)){
# Calling In Bear Data
  # Set the path to pull data from the correct folder
setwd(path.data)
  # Defining the name of the bear from the base name of the each file in your list
name<- basename(file.list[i])
 # Pulling in the actual data using the list of file names
bear<-read.csv(paste0(file.list[i], sep=''))



# Calling In Organized Excel
 # Set the path to pull data from the correct folder
setwd(path.info)
 # This file holds information about the bears reproductive status, fix interval, and den exit and emergence
all.bear.info <- read.csv("Bear_Files_Organization.csv")

# Get Rid of Rows without Data
  # This deletes any row in the bear file that does not have GPS information
bear <- bear[!is.na(bear$GPS.Latitude),]


# Pull out bear row from organization file
bear.info <- all.bear.info[all.bear.info$bear == name, ]
bear.info

# Compare Columns for the Bear Info and Bear File
# Note that there are separate date and time columns in both
head(all.bear.info)
bear.info$den_exit <- paste(bear.info$den_exit_day, bear.info$den_exit_time_UTC)
bear.info$den_entry <- paste(bear.info$den_entry_day, bear.info$den_entry_time_UTC)


# Combine date and time columns and truncate for den
# Bear.info file
# Pull out values for exit and entry
den_exit <- bear.info$den_exit
den_entry <- bear.info$den_entry

# Removing Rows Before Den Emergence/Exit
bear <- bear[bear$GPS.Fix.Time >= den_exit, ]

# Removing Rows After Den Entry
bear <- bear[bear$GPS.Fix.Time <= den_entry, ]



# Deal with Date and Time
# The times for the data points need to be converted into UTC
datetime.est <-as.POSIXct(strptime(as.character(bear$GPS.Fix.Time), tz="UTC", "%Y.%m.%d %H:%M:%S"))
datetime.est <- format(datetime.est, tz = "America/New_York", usetz = TRUE)

bear$GPS.Fix.Time <- paste(datetime.est)



# Deal with Location Error
# Make sure the column names are the same for both types of collars 
# For all collars, row 14 has the dilution measurement
names(bear)[14]<-paste("GPS.Positional.Dilution")

# This gets rid of all of the rows that have location data that may be errored
bear<-subset(bear,((bear$GPS.Fix.Attempt=="Resolved QFP (Uncertain)"|bear$GPS.Fix.Attempt=="Succeeded (2D)")& 
                     bear$GPS.Positional.Dilution< 5)|
               ((bear$GPS.Fix.Attempt=="Succeeded (3D)"|bear$GPS.Fix.Attempt=='Resolved QFP')&
               bear$GPS.Positional.Dilution<20))


#Adding Seasons
# Season breakpoints in month-day format
Season.breaks<-as.Date(c("01-01","03-14","06-14","08-10","11-15","12-31"),format = "%m-%d")
 
# Convert season breakpoints to Julian day in numeric format
Season.breaks<-as.numeric(format(Season.breaks,format="%j"))

# Make a lookup table using a function in spatstat package
# Season.lut is a function that can be used in apply or a loop
# This step defines the season break dates an a categorical season
library(spatstat)
season.lut<-lut(as.factor(c("Winter","Spring","Summer","Fall","Winter")),breaks = Season.breaks)

# Make a new column with the fix dates without the exact time. This will be in the format Year-Month-Day
bear$short.fix.time <- bear$GPS.Fix.Time
bear$short.fix.time <- as.Date(bear$short.fix.time)

 
# Add a column to the data for season using the season.lut lookup table function
bear$season <- season.lut(as.numeric(format(bear$short.fix.time,format="%j")))

# Remove winter rows from data
bear <- bear[bear$season != "Winter", ]

# Remove rows with NAs for season
bear <- bear[!is.na(bear$season),]

# Add Bear Info to Bear File
bear$bear <- bear.info$bear
bear$name <- bear.info$name
bear$bear_number <- bear.info$bear_number
bear$year <- bear.info$year
bear$cub_status <- bear.info$cub_status

# Rename "GPS.Positional.Dilution" to "GPS.Dilution"  
bear$GPS.Dilution <- bear$GPS.Positional.Dilution


# Make Columns Consistent with Other Collars
# GPS Fix Time, GPS Latitude, GPS Longitude

# Select Columns
keep.col <- c("bear", "name", "bear_number", "year", "cub_status", "GPS.Fix.Time", "GPS.Latitude", "GPS.Longitude", "GPS.Fix.Attempt", "GPS.Dilution", "season")
bear <- bear[keep.col]

# When it is finalized, save to "Bear Data Cleaned"
path.cleaned <- "C:/Users/jessi/University of Massachusetts/Jessica-BearProject - General/Final Data/Bear Data Cleaned"
setwd(path.cleaned)
fname=as.character(paste(file.list[i]))
#create output file with above name
write.csv(bear, file = fname, row.names = FALSE)
}
```

### Removing Extreme Movements 

Some of the bears took unusual movement paths while collared. The locations of these unusual movements can affect habitat analysis because it differs from normal patterns of behavior. We are not sure why this behavior occurs, but it could be due to a unique pressure put on an individual. For this reason, the data during those extreme movement patterns are removed from the data. The dates for the data being removed were determined manually by identifying the start and finish of the extreme movement path in ArcGIS.


**Libraries**
Installing and using the right packages is important to ensure your functions run properly.
```{r}
library(dplyr)
```

**Set Paths**
Setting paths is important to make sure you are working with data in the correct file folder.
It will vary based on where your personal data is stored.
```{r}
path.root <- "C:/Users/jessi/University of Massachusetts/Jessica-BearProject - General/Final Data"
path.data <- paste(path.root, "/Bear Data Cleaned", sep = "")
path.ex.mov <- paste(path.root, "/Bear Information", sep = "")
path.save <- paste(path.root, "/Bear Data Cleaned", sep = "")
```

**Call in Extreme Movement Data**
This file was manually created with the dates of the extreme movements identified in ArcGIS.
```{r}
# Set the path to pull data from the correct folder
setwd(path.ex.mov)
# This file has the date and time for when each of the bears start and end their extreme movement path
ex_movements <- read.csv("Extreme_Movements.csv")
```

**Call in Bears with Extreme Movements**
```{r}
# Set the path
setwd(path.data)

# Call in .csvs for the bears that were determined to make an unusual movement path
BigPrescott2017 <- read.csv("BigPrescott2017.csv")
Deck2021 <- read.csv("Deck2021.csv")
Emily2017 <- read.csv("Emily2017.csv")
Jersey2017 <- read.csv("Jersey2017.csv")
July2017 <- read.csv("July2017.csv")
Pelham465_2018 <- read.csv("Pelham465_2018.csv")
Sunderland2017 <- read.csv("Sunderland2017.csv")
Swift2017 <- read.csv("Swift2017.csv")
Templeton2017 <- read.csv("Templeton2017.csv")
Templeton2021 <- read.csv("Templeton2021.csv")
```

**List the Extreme Movements for Each Bear**
```{r}
# List the Extreme Movements for Each Bear
list(ex_movements)
```

**Get Rid of the Rows that Fall Between the Start and End of BigPrescott2017 Extreme Movement Path**
```{r}
# Determine which rows are the start and end
which(BigPrescott2017$GPS.Fix.Time == '2017-09-26 14:45:39 EDT')
which(BigPrescott2017$GPS.Fix.Time == '2017-10-18 17:00:43 EDT')

# Remove the rows that fall in the extreme movements 
BigPrescott2017 <- BigPrescott2017[-(3526:3950),]
```

**Get Rid of the Rows that Fall Between the Start and End of Deck2021 Extreme Movement Path**
```{r}
# Determine which rows are the start and end
which(Deck2021$GPS.Fix.Time == '2021-10-07 13:15:21 EDT')
which(Deck2021$GPS.Fix.Time == '2021-11-20 17:30:11 EST')

# Remove the rows that fall in the extreme movements
Deck2021 <- Deck2021[-(5586: 7010),]
```

**Get Rid of the Rows that Fall Between the Start and End of Emily2017 Extreme Movement Path**
```{r}
# Determine which rows are the start and end
which(Emily2017$GPS.Fix.Time == '2017-10-15 08:45:37 EDT')
which(Emily2017$GPS.Fix.Time == '2017-11-06 15:15:37 EST')

# Remove the rows that fall in the extreme movements
Emily2017 <- Emily2017[-(3907:4358),]
```

**Get Rid of the Rows that Fall Between the Start and End of Jersey2017 Extreme Movement Path**
```{r}
# Determine which rows are the start and end
which(Jersey2017$GPS.Fix.Time == '2017-10-04 21:30:24 EDT')
which(Jersey2017$GPS.Fix.Time == '2017-10-18 21:30:19 EDT')

# Remove the rows that fall in the extreme movements
Jersey2017 <- Jersey2017[-(2123:2415),]
```

**Get Rid of the Rows that Fall Between the Start and End of July2017 Extreme Movement Path**
```{r}
# Determine which rows are the start and end
which(July2017$GPS.Fix.Time == '2017-09-20 17:45:31 EDT')
which(July2017$GPS.Fix.Time == '2017-11-13 11:30:20 EST')

# Remove the rows that fall in the extreme movements
July2017 <- July2017[-(3587:4762),]
```

**Get Rid of the Rows that Fall Between the Start and End of Pelham465_2018 Extreme Movement Path**
```{r}
# Determine which rows are the start and end
which(Pelham465_2018$GPS.Fix.Time == '2018-09-13 21:30:43 EDT')
which(Pelham465_2018$GPS.Fix.Time == '2018-11-23 04:00:37 EST')

# Remove the rows that fall in the extreme movements
Pelham465_2018 <- Pelham465_2018[-(3311:4815),]
```

**Get Rid of the Rows that Fall Between the Start and End of Sunderland2017 Extreme Movement Path**
```{r}
# Determine which rows are the start and end
which(Sunderland2017$GPS.Fix.Time == '2017-10-03 23:45:25 EDT')
which(Sunderland2017$GPS.Fix.Time == '2017-10-21 04:15:22 EDT')

# Remove the rows that fall in the extreme movements
Sunderland2017 <- Sunderland2017[-(4154:4551),]
```

**Get Rid of the Rows that Fall Between the Start and End of Swift2017 Extreme Movement Path**
```{r}
# Determine which rows are the start and end
which(Swift2017$GPS.Fix.Time == '2017-09-06 20:45:39 EDT')
which(Swift2017$GPS.Fix.Time == '2017-10-10 14:00:25 EDT')

# Remove the rows that fall in the extreme movements
Swift2017 <- Swift2017[-(2761:3255),]
```

**Get Rid of the Rows that Fall Between the Start and End of Templeton2017 Extreme Movement Path**
```{r}
# Determine which rows are the start and end
which(Templeton2017$GPS.Fix.Time == '2017-10-02 18:30:38 EDT')
which(Templeton2017$GPS.Fix.Time == '2017-12-23 19:45:30 EST')

# Remove the rows that fall in the extreme movements
Templeton2017 <- Templeton2017[-(1695:3353),]
```

**Get Rid of the Rows that Fall Between the Start and End of Templeton2021 Extreme Movement Path**
```{r}
# Determine which rows are the start and end
which(Templeton2021$GPS.Fix.Time == '2021-09-13 13:15:10 EDT')
which(Templeton2021$GPS.Fix.Time == '2021-11-07 12:15:47 EST')

# Remove the rows that fall in the extreme movements
Templeton2021 <- Templeton2021[-(5433:7190),]
```

**Save to replace**
```{r}
# Set the path to pull data from the correct folder
setwd(path.save)
# Save the files with the same name to replace the existing files that include the extreme movement rows
write.csv(BigPrescott2017, file = "BigPrescott2017.csv", row.names = FALSE)
write.csv(Deck2021, file = "Deck2021.csv", row.names = FALSE)
write.csv(Emily2017, file = "Emily2017.csv", row.names = FALSE)
write.csv(Jersey2017, file = "Jersey2017.csv", row.names = FALSE)
write.csv(July2017, file = "July2017.csv", row.names = FALSE)
write.csv(Pelham465_2018, file = "Pelham465_2018.csv", row.names = FALSE)
write.csv(Sunderland2017, file = "Sunderland2017.csv", row.names = FALSE)
write.csv(Swift2017, file = "Swift2017.csv", row.names = FALSE)
write.csv(Templeton2017, file ="Templeton2017.csv", row.names = FALSE)
write.csv(Templeton2021, file ="Templeton2021.csv", row.names = FALSE)
```

## Creating Shapefiles {.tabset}

After the data are cleaned, shapefiles are created to have the data as spatially referenced. This takes the data for each GIS location and assigns it to a spatially referenced point. For this project, we want all data to be referenced to UTM Zone 18 North. This data was collected using World Geodetic System 1984. Note: CRS stands for Coordinate Reference System.


**Set Paths**
Set the path pick the file folder to pull in your data. It will be different based on where your data is stored.
```{r}
path.root <- "C:/Users/jessi/University of Massachusetts/Jessica-BearProject - General/Final Data"
path.data <- paste(path.root, "/Bear Data Cleaned", sep = "")
path.save <- paste(path.root, "/Cleaned Bear Data Shapefiles", sep = "")
```

**Libraries**
Make sure to install and pull in the libraries you need in order for your code functions to work.
```{r}
require(sf)
require(dplyr)
require(psych)
library(raster)
library(sp)
library(dismo)
library(stringr)
require(stars)
library(rgdal)
library(readxl)
```

**Define Projection Strings**
The projection string tells R what projection your data is in. If the projection is wrong or not defined, the data will not be graphically located correctly. 
```{r}
# Assign the projection string
# WGS84 - World Geodetic System 1984
# The data was collected referencing this projection
prj.wgs84 <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs" 
LatLong <- crs(prj.wgs84)

# WGS 84 / UTM zone 18N
# We want the final data to be referencing this projection
prj.utmWGSz18 <- "+proj=utm +zone=18 +ellps=WGS84 +datum=WGS84 +units=m +no_defs"
UTM18 <- crs(prj.utmWGSz18)
```

**List All Bear Files**
This allows you to pull in all of the file names from one file folder. It is important to note that this is not the data itself, just the names of the files in the folder.
```{r}
setwd(path.data)
# Pulling in by the pattern lists every file in the folder with that syntax
# In this case we are using the file extension 
file.list <- list.files(pattern = "\\.csv$", full.names = FALSE)
file.list
```

**Creating the Shapefiles From .csv Data**
Doing this in a loop allows R to run the code for each file one after the other.
```{r}
for (i in 1:length(file.list)){
# Calling In Bear Data
# Set the path to pull data from the correct folder
setwd(path.data)
# Defining the name of the bear from the base name of the each file in your list
name<- basename(file.list[i])
# Pulling in the actual data using the list of file names 
bear<-read.csv(paste0(file.list[i], sep=''))


# Convert the .csv files to an spatial frame object and define its projection
bear <- st_as_sf(bear, coords = c("GPS.Longitude", "GPS.Latitude"), crs = LatLong)
st_crs(bear)


# Transform into UTM Zone 18 N
# The EPSG code form UTM Zone 18 North is 32618
bear_UTM <- st_transform(bear, crs = UTM18)
st_crs(bear_UTM) <- 32618


# Export As a Shapefile
setwd(path.save)
fname=as.character(paste(file.list[i]))
fname = paste(fname, sep = "_")
fname <- gsub('.csv', '', fname)
st_write(bear_UTM, fname, driver = "ESRI Shapefile", append=FALSE)
}
```


## MCPs

The home range of each bear will be determined using a Minimum Convex Polygon. We will generate a 99% MCP (meaning generating a border around 99% of the data points). This ensures that if any extreme fixes were not removed during data cleaning, the home range calculation will not include the area where that point lies. We will also include a 200-meter buffer around the MCP. This is to include the area around the use locations that is still available for the bear to go. We chose 200 meters because it is the average step length between the 45-minute time intervals.


**Libraries**
Installing and using the right packages is important to ensure your functions run properly.
```{r}
library(adehabitatHR)
library(sp)
library(rgdal)
library(sf)
```

**Set Paths**
Setting paths is important to make sure you are working with data in the correct file folder. It will vary based on where your personal data is stored.
```{r}
path.root <- "C:/Users/jessi/University of Massachusetts/Jessica-BearProject - General/Final Data"
path.data <- paste(path.root, "/Bear Data Cleaned", sep = "")
```

**List All Bear Files**
This allows you to pull in all of the file names from one file folder. It is important to note that this is not the data itself, just the names of the files in the folder.
```{r}
setwd(path.data)
file.list <- list.files(pattern = "\\.csv$", full.names = FALSE)
file.list
```

**Creating the Loop to convert data into UTM Zone 18 North and create the MCP with a buffer**
Doing this in a loop allows R to run the code for each file one after the other.
```{r}
for (i in 1:length(file.list)){
# Calling In Bear Data
  # Set the path to pull data from the correct folder
setwd(path.data)
# Defining the name of the bear from the base name of the each file in your list
name<- basename(file.list[i])
 # Pulling in the actual data using the list of file names
bear<-read.csv(paste0(file.list[i], sep=''))

# Make a function to convert WGS84 to UTM
# Data was collected in reference to WGS84
LongLatToUTM<-function(x,y,zone){
 xy <- data.frame(ID = 1:length(x), X = x, Y = y)
 coordinates(xy) <- c("X", "Y")
 proj4string(xy) <- CRS("+proj=longlat +datum=WGS84") 
 res <- spTransform(xy, CRS(paste("+proj=utm +zone=18 +ellps=WGS84 +datum=WGS84 +units=m +no_defs",sep='')))
 return(as.data.frame(res))
}
# Define the longitude and latitude as x and y
x <- bear$GPS.Longitude
y <- bear$GPS.Latitude
# Use the function to convert to UTM
bear.sp = LongLatToUTM(x,y,18)
# Change the column names
colnames(bear.sp) <- c("name", "X", "Y")
bear.sp$name = name
# Tell R that the X (longitude) and Y (latitude) are the coordinates for the data
coordinates(bear.sp) <- c("X", "Y")
# Define the coordinate reference system to UTM
proj4string(bear.sp) <- CRS("+proj=utm +zone=18 +ellps=WGS84 +datum=WGS84 +units=m +no_defs")



# Generate the MCP
bear.mcp <- mcp(bear.sp, percent = 99)
bear.mcp
# Define the projection string as UTM Zone 18 N
proj4string(bear.mcp) <- CRS("+proj=utm +zone=18 +ellps=WGS84 +datum=WGS84 +units=m +no_defs")

# Add a 200 meter buffer
library(rgeos)
buffer <- gBuffer(bear.mcp, width = 200, byid = T)
proj4string(buffer) <- CRS("+proj=utm +zone=18 +ellps=WGS84 +datum=WGS84 +units=m +no_defs")

# Export As a Shapefile
setwd(path.root)
fname=as.character(paste(file.list[i]))
fname <- gsub('.csv', '', fname)
writeOGR(buffer, "MCPs", fname, driver = "ESRI Shapefile", check_exists=TRUE, overwrite_layer = TRUE)
}
```


## Generating Random Points {.tabset}

Random points are generated to assess available resources.


### Density of Points

To determine how many random points would accurately represent availability we calculated a density of points using 9 bear MCPs. For these 9 bears, we determined the minimum number of points to accurately represent land use within the home range. We calculated the density of points for each bear by dividing the minimum number of points by the area of the home range. The average density of points was determined to be 105.8 points per square kilometer.


**Libraries**
Installing and using the right packages is important to ensure your functions run properly.
```{r}
require(sf)
require(dplyr)
require(psych)
library(raster)
library(sp)
library(dismo)
library(stringr)
require(stars)
```

**Set Paths**
Setting paths is important to make sure you are working with data in the correct file folder. It will vary based on where your personal data is stored.
```{r}
path.root <- "C:/Users/jessi/University of Massachusetts/Jessica-BearProject - General/Final Data"
path.data <- paste(path.root, "/Bear Data Cleaned", sep = "")
path.mcp.data <- paste(path.root, "/MCPs", sep = "")
```

**Pull in MCPs of bears used for exploratory analysis**
```{r}
setwd(path.mcp.data)
Amy2012_MCP <- read_sf("Amy2012.shp")
Bittersweet2017_MCP <- read_sf("Bittersweet2017.shp")
BlackHawk2020_MCP <- read_sf("BlackHawk2020.shp")
Bonnie2013_MCP <- read_sf("Bonnie2013.shp")
Granby2_2018_MCP <- read_sf("Granby2_2018.shp")
LittlePrescott2015_MCP <- read_sf("LittlePrescott2015.shp")
Shirley2020_MCP <- read_sf("Shirley2020.shp")
Solar2019_MCP <- read_sf("Solar2019.shp")
Templeton2018_MCP <- read_sf("Templeton2018.shp")
```

**Convert area from meters squared to sq km**
Divide the area value by 1e+6
```{r}
# Define the crs as UTM Zone 18 North then convert MCP area to sq Km
st_crs(Amy2012_MCP) <- 32618
Amy2012_MCP$area_km_sq <- (st_area(Amy2012_MCP)/1000000)
# Define the crs as UTM Zone 18 North then convert MCP area to sq Km
st_crs(Bittersweet2017_MCP)<- 32618
Bittersweet2017_MCP$area_km_sq <- (st_area(Bittersweet2017_MCP)/1000000)
# Define the crs as UTM Zone 18 North then convert MCP area to sq Km
st_crs(BlackHawk2020_MCP)<- 32618
BlackHawk2020_MCP$area_km_sq <- (st_area(BlackHawk2020_MCP)/1000000)
# Define the crs as UTM Zone 18 North then convert MCP area to sq Km
st_crs(Bonnie2013_MCP)<- 32618
Bonnie2013_MCP$area_km_sq <- (st_area(Bonnie2013_MCP)/1000000)
# Define the crs as UTM Zone 18 North then convert MCP area to sq Km
st_crs(Granby2_2018_MCP)<- 32618
Granby2_2018_MCP$area_km_sq <- (st_area(Granby2_2018_MCP)/1000000)
# Define the crs as UTM Zone 18 North then convert MCP area to sq Km
st_crs(LittlePrescott2015_MCP)<- 32618
LittlePrescott2015_MCP$area_km_sq <- (st_area(LittlePrescott2015_MCP)/1000000)
# Define the crs as UTM Zone 18 North then convert MCP area to sq Km
st_crs(Shirley2020_MCP)<- 32618
Shirley2020_MCP$area_km_sq <- (st_area(Shirley2020_MCP)/1000000)
# Define the crs as UTM Zone 18 North then convert MCP area to sq Km
st_crs(Solar2019_MCP)<- 32618
Solar2019_MCP$area_km_sq <-(st_area(Solar2019_MCP)/1000000)
# Define the crs as UTM Zone 18 North then convert MCP area to sq Km
st_crs(Templeton2018_MCP)<- 32618
Templeton2018_MCP$area_km_sq <- (st_area(Templeton2018_MCP)/1000000)
```

**Create a matrix to put minimum number of points, MCP area, and density of points calculated**
```{r}
min_points <- matrix(0, nrow = 9, ncol = 3)
# Make the row names the bear
rownames(min_points) <- c("Amy2012","Bittersweet2017","BlackHawk2020","Bonnie2013","Granby2_2018","LittlePrescott2015","Shirley2020","Solar2019","Templeton2018")
# Define the column names
colnames(min_points) <- c("Min_no_Pts", "area", "Den_of_Pts")
```

**Add the predetermined minimum number of points to the first row of the matrix**
```{r}
# Amy2012
min_points[1,1] <- 6000
# Bittersweet2017
min_points[2,1] <- 6000
# BlackHawk2020
min_points[3,1] <- 6000
# Bonnie2013
min_points[4,1] <- 4500
# Granby2_2018
min_points[5,1] <- 3500
# LittlePrescott2015
min_points[6,1] <- 8000
# Shirley2020
min_points[7,1] <- 4000
# Solar2019
min_points[8,1] <- 6000
# Templeton2018
min_points[9,1] <- 8000
```

**Add the area of each home range to the second column of the matrix**
```{r}
min_points[1,2] <- Amy2012_MCP$area_km_sq
min_points[2,2] <- Bittersweet2017_MCP$area_km_sq
min_points[3,2] <- BlackHawk2020_MCP$area_km_sq
min_points[4,2] <- Bonnie2013_MCP$area_km_sq
min_points[5,2] <- Granby2_2018_MCP$area_km_sq
min_points[6,2] <- LittlePrescott2015_MCP$area_km_sq
min_points[7,2] <- Shirley2020_MCP$area_km_sq
min_points[8,2] <- Solar2019_MCP$area_km_sq
min_points[9,2] <- Templeton2018_MCP$area_km_sq
```

**Calculate Density of Points for Each Bear by dividing the minimum number of points by the area of the home range**
 This will go into the third column of the matrix
```{r}
min_points <- as.data.frame(min_points)
# Density of points = min number of points/area
min_points$Den_of_Pts <- ((min_points$Min_no_Pts)/(min_points$area))
min_points
```

**Calculate the average Density of Points for the 9 bears**
```{r}
# Calculate the mean density of points
Density_of_Points <- mean(min_points$Den_of_Pts)
print(Density_of_Points)
```


### Generating

Random points need to be generated for each MCP to determine what is available for the bear to use. RSFs compare use points (GPS locations) to available points (random points in each MCP) to determine selection. We used a density of points for each MCP to determine how many random points would accurately represent availability. The density of points was determined to be 105.8 Points per square kilometer.


**Libraries**
Installing and using the right packages is important to ensure your functions run properly.
```{r}
require(sf)
require(dplyr)
require(psych)
library(raster)
library(sp)
library(dismo)
library(stringr)
require(stars)
library(rgdal)
library(readxl)
```

**Set Paths**
Setting paths is important to make sure you are working with data in the correct file folder. It will vary based on where your personal data is stored. 
```{r}
path.root <- "C:/Users/jessi/University of Massachusetts/Jessica-BearProject - General/Final Data"
path.mcp.data <- paste(path.root, "/MCPs", sep = "")
path.info <- paste(path.root, "/Bear Information", sep = "")
path.save <- paste(path.root, "/Availability Points Shapefiles", sep = "")
```

**List All Bear Files**
This allows you to pull in all of the file names from one file folder. It is important to note that this is not the data itself, just the names of the files in the folder.
```{r}
setwd(path.mcp.data)
# Pulling in by the pattern lists every file in the folder with that syntax
# In this case we are using the file extension 
file.list <- list.files(pattern = "\\.shp$", full.names = FALSE)
file.list
```

**Calling in bear info**
This file contains basic information including bear, name, bear_number, year, and cub_status
```{r}
setwd(path.info)
bear.info <- read_excel("Bear_Files_Summary.xlsx")
```

**Create a matrix to put the calculated number of random points needed for each bear**
```{r}
# Create matix
no_of_rand_points <- matrix(0, length(file.list), ncol = 3)
# Define the column names
colnames(no_of_rand_points) <- c("Bear","Area", "No_of_Rand_Pts")
```

**Calculate Number of Random Points for Each MCP**
Density of Points = 105.8 Points Per Sq Km
```{r}
for (i in 1:length(file.list)){
# Calling In MCP Data
setwd(path.mcp.data)
# Defining the name of the bear from the base name of the each file in your list
name <- basename(file.list[i])
# Using gsub will delete the file extension from the name 
name <- gsub('.shp', '', name)
mcp <- read_sf(paste0(file.list[i], sep=''))


# Define projection of the MCPs
st_crs(mcp) <- 32618
# Convert area from meters squared to sq km
# Divide the area value by 1e+6
mcp$area_km_sq <- (st_area(mcp)/1000000)

# Add the information to the maxtrix created earlier
# Column 1: The name if the bear with year
no_of_rand_points[i,1] <- name
# Column 2: The area in square kilometers
no_of_rand_points[i,2] <- mcp$area_km_sq
# Column 3: The number of points to generate. Calculated by multiplying the area by the density of points
no_of_rand_points[i,3] <- (mcp$area_km_sq*105.8)

# List as numeric
rand_pts <- as.numeric(no_of_rand_points[i,3])

# Generate Random Points

# Rasterize the MCP
mcp_R <- st_rasterize(mcp %>% dplyr::select(geometry))
mcp_R <- as(mcp_R, "Raster")

# Create Random Points
bear_RP <- randomPoints(mcp_R, rand_pts)
bear_RP <- as.data.frame(bear_RP)

# Define x and y into environments
x <- bear_RP$x
y <- bear_RP$y
# Convert the random points tp spatial frame object
bear_RP <- st_as_sf(x = bear_RP, coords = c("x", "y"), crs = "+proj=utm +zone=18 +ellps=WGS84 +datum=WGS84 +units=m +no_defs")

# Add all columns to match use data
# Subset the bear info by pulling out the row for the bear iteration 
bear_info <- bear.info[bear.info$bear == name,]

# Add that information to the generated random points
bear_RP <-cbind(bear_RP,bear_info)

# Export As a Shapefile
setwd(path.save)
fname = as.character(paste(file.list[i]))
RP <- "RP"
fname = paste(RP, fname, sep = "_")
fname <- gsub('.csv', '', fname)
st_write(bear_RP, fname, driver = "ESRI Shapefile", append=FALSE)
}
```


## Getting Points Ready for Analysis {.tabset}

There are some final steps that need to be taken to make sure the data are in proper format in order to run an RSF.

### Combining Points Shapefiles

Combining the shapefiles of each individual file will allow for easier spatial analysis. All of the individual's information will be kept in the combined files. This way, if you do want to pull information from a specific bear, year, or file, it is still possible to do so.


**Set Paths**
Set the path pick the file folder to pull in your data. It will be different based on where your data is stored.
```{r}
path.root <- "C:/Users/jessi/University of Massachusetts/Jessica-BearProject - General/Final Data"
path.use.data <- paste(path.root, "/Bear Data Cleaned", sep = "")
path.RP.data <- paste(path.root, "/Availability Points Shapefiles", sep = "")
path.save <- paste(path.root, "/Combined Data/Combined Data", sep = "")
```

**Libraries**
Make sure to install and pull in the libraries you need in order for your code functions to work.
```{r}
library(readr)
library(raster)
library(rgdal)
library(sf)
```

**List All Use Bear Files**
This allows you to pull in all of the file names from one file folder. It is important to note that this is not the data itself, just the names of the files in the folder.
```{r}
setwd(path.use.data)
# Pulling in by the pattern lists every file in the folder with that syntax
# In this case we are using the file extension 
file.list <- list.files(pattern = "\\.csv$", full.names = FALSE)
file.list
```

**Pull in all of those files to the environments**
```{r}
# Set path
setwd(path.use.data)
df <- readr::read_csv(file.list, id = "file_name")
df

# Add a new column to have the bear as NameYear
# using gsub removes the file extension for each row in the bear column
df$bear<- gsub('.csv', '',df$bear)

# Get rid of first column which is the file name with the file extension of .csv
df <- df[, 2:12]
```

**Make File into Shapefile**
```{r}
# Assign the projection string
# WGS84 - World Geodetic System 1984
prj.wgs84 <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs" 
LatLong <- crs(prj.wgs84)

# WGS 84 / UTM zone 18N
prj.utmWGSz18 <- "+proj=utm +zone=18 +ellps=WGS84 +datum=WGS84 +units=m +no_defs"
UTM18 <- crs(prj.utmWGSz18)

# Convert the .csv files to an spatial frame object and define its projection
shapefile <- st_as_sf(df, coords = c("GPS.Longitude", "GPS.Latitude"), crs = LatLong)
st_crs(shapefile)


# Transform into UTM Zone 
shapefile <- st_transform(shapefile, crs = UTM18)
st_crs(shapefile) <- 32618
```

**Repeat these steps for the random points (available) data**
```{r}
setwd(path.RP.data)
# Pulling in by the pattern lists every file in the folder with that syntax
# In this case we are using the file extension 
file.list.RP <- list.files(pattern = "\\.shp$", full.names = FALSE)
file.list.RP


# Loop through the list of .shp files and read them into a list of sf objects
sf_list <- lapply(file.list.RP, st_read)

# You can access each individual sf object in the list using list indexing, e.g. sf_list[[1]] for the first file

# Optionally, you can also combine all the sf objects into a single sf object using `do.call()` and `rbind()`
sf_combined <- do.call(rbind, sf_list)


# Rename columns to match the use data
names(sf_combined)[names(sf_combined) == 'br_nmbr'] <- 'bear_number'
names(sf_combined)[names(sf_combined) == 'cb_stts'] <- 'cub_status'

# Define the coordinate system as UTM zone 18N
st_crs(sf_combined) <- 32618
```

**Plot to Double Check**
```{r}
setwd("C:/Users/jessi/University of Massachusetts/Jessica-BearProject - General/Final Data/GIS Layers")
mass_outline <- st_read("mass_outline.shp")

plot(mass_outline$geometry)
plot(shapefile$geometry, add = TRUE, col = "blue")
plot(sf_combined$geometry, add = TRUE, col = "red")
```

**Save All Combined Data**
```{r}
# Set path to where you would like to save the outputs
setwd(path.save)

# Save Use Points data frame
write.csv(df, file = "Use_Data.csv")

# Save Use Points Shapefile
st_write(shapefile, "Use_Points", driver = "ESRI Shapefile", append=FALSE)

# Save Available Points Shapefile
st_write(sf_combined, "Available_Points", driver = "ESRI Shapefile", append=FALSE)
```


### Clipping Data To MA

The data needs to be clipped to the Massachusetts state boundary. This is because the GIS layers used in this project are all in the extent of the MA border. If points were to extract spatial data outside the MA border, the results would be NA because there isn't spatial data outside the state.  


**Set Paths**
Set the path pick the file folder to pull in your data. It will be different based on where your data is stored. 
```{r}
path.root <- "C:/Users/jessi/University of Massachusetts/Jessica-BearProject - General/Final Data"
path.use.data <- paste(path.root, "/Combined Data/Combined Data/Use_Points", sep = "")
path.RP.data <- paste(path.root, "/Combined Data/Combined Data/Available_Points", sep = "")
path.mass.outline <- paste(path.root, "/GIS Layers", sep = "")
path.save <- paste(path.root, "/Combined Data/Combined Data Clipped to MA", sep = "")
```

**Libraries**
Make sure to install and pull in the libraries you need in order for your code functions to work.
```{r}
library(readr)
library(raster)
library(rgdal)
library(sf)
library(dplyr)
```

**Pull in the Use Data shapefile, available data shapefile, and the MA outline shapefile**
```{r}
# Use Data
setwd(path.use.data)
use_points <- st_read("Use_Points.shp")

# Available Data
setwd(path.RP.data)
avail_points <- st_read("Available_Points.shp")

# Massachusetts Outline
setwd(path.mass.outline)
ma_outline <- st_read("mass_outline.shp")
```

**Plot**
```{r}
plot(ma_outline$geometry)
plot(avail_points$geometry, add = TRUE, col = "blue", pch = 20)
plot(use_points$geometry, add = TRUE, col = "red", pch = 20)
```

**List the Counts of Use and Available points**
This will be important later to see how many points are lost by when the data are cropped to the state.
```{r}
# Count of use data points (578048)
use_count_before <- nrow(use_points)

# Count of available data points (i409868)
avail_count_before <- nrow(avail_points)
```

**Clip the data points by the mass outline**
```{r}
use_clip <- use_points[ma_outline, ]

avail_clip <- avail_points[ma_outline, ]
```

**Check to see how much data was lost**
```{r}
# Count of use data points 
use_count_after <- nrow(use_clip)

# Count of available data points 
avail_count_after <- nrow(avail_clip)

# Find out how many points were lost in the process
print(use_count_before - use_count_after)

print(avail_count_before - avail_count_after)
```

**Plot to Double Check that the points data is within the mass border**
```{r}
plot(ma_outline$geometry)
plot(avail_clip$geometry, add = TRUE, col = "blue", pch = 20)
plot(use_clip$geometry, add = TRUE, col = "red", pch = 20)
```

**Save**
```{r}
# Set the path to where you want your outputs saved
setwd(path.save)

# Save Use Points Shapefile
st_write(use_clip, "Use_Points", driver = "ESRI Shapefile", append=FALSE)

# Save Available Points Shapefile
st_write(avail_clip, "Available_Points", driver = "ESRI Shapefile", append=FALSE)
```


## Extracting Data {.tabset}

Extracting distance to nearest structure was done using the "near" tool in ArcGIS.This step was done after clipping to MA and data were saved as shapefiles in "Combined, Clipped, Near" folder.


### Urbanization Gradient

To explore if bears selection changes across varying levels of urbanization we calculated the percent impervious surface plus developed open space for each bearâ€™s home range. This is one of the variables that will go into the model.

**Libraries**
Make sure to install and pull in the libraries you need in order for your code functions to work.
```{r}
library(raster)
library(sp)
library(adehabitatHR)
library(adehabitatHS)
library(reshape2)
library(sf)
library(writexl)
```

**Set Paths**
Setting paths is important to make sure you are working with data in the correct file folder. It will vary based on where your personal data is stored.
```{r}
path.root <- "C:/Users/jessi/University of Massachusetts/Jessica-BearProject - General/Final Data"
path.mcp <- paste(path.root, "/MCPs", sep = "")
path.GIS <- paste(path.root, "/GIS Layers", sep = "")
``` 

**Bring in the land use data**
```{r}
setwd(path.GIS)
lulc2016 <-raster("LULC2016.tif")
```

**Examine the raster data**
```{r}
# Check the coordinate reference system
print(crs(lulc2016))

# Define the projection string for UTM Zone 18
UTM18 <- CRS("+init=epsg:32618")

# Define land use coordinate system
crs(lulc2016) <- UTM18
print(crs(lulc2016))

# Plot to look at data
plot(lulc2016)
```

**Reclassify map into fewer land cover categories**
```{r}
setwd(path.GIS)
classification <- read.csv("Land_Cover_Reclass.csv")
head(classification)
class <- as.matrix(classification[,c(1,3)])
land_sub <- reclassify(lulc2016, rcl = class)
plot(land_sub)
```

**Create a data frame**
```{r}
data <- data.frame()
```

**List All MCP Files**
This allows you to pull in all of the file names from one file folder. It is important to note that this is not the data itself, just the names of the files in the folder.
```{r}
setwd(path.mcp)
file.list <- list.files(pattern = "\\.shp$", full.names = FALSE)
file.list
```

**Calculate the percent developed for each MCP** Extract all of the raster values in each MCP. Then add the number of raster values of 5 (impervious surface) and 4 (developed open space). Divide that by the total number of values and multiply by 100.
```{r}
for (i in 1:length(file.list)){
# Calling In MCP Data
setwd(path.mcp)
# Defining the name of the bear from the base name of the each file in your list
bear <- basename(file.list[i])
# Using gsub will delete the file extension from the name 
bear <- gsub('.shp', '', bear)
mcp <- read_sf(paste0(file.list[i], sep=''))

# Convert
mcp <- sf:::as_Spatial(mcp$geometry)

# Extract values of raster cells within polygon
raster_values <- raster::extract(land_sub, mcp)

# Convert raster values to raster layer
raster_values <- unlist(raster_values)

# Remove any values of na
raster_values <- na.omit(raster_values)

# Calculate the percent of impervious surface (raster value = 5) plus developed open space (raster value = 4) 
Percent_Impervious <- (length(raster_values[(raster_values == 5)|(raster_values == 4)]))/(length(raster_values))*100


output = c(bear, Percent_Impervious)
data = rbind(data, output)
colnames (data) <- c("Bear", "Percent_Developed")
}
```

**Make sure that the columns are the correct class**
```{r}
# Copy column and remove last 4 digits (Year)
data$name <- substr(data$Bear, 1, nchar(data$Bear) - 4)

data$name <- as.character(data$name)

data$Percent_Developed <- as.numeric(data$Percent_Developed)
```

**Save** Save this information as an excel in order to bring in later.
```{r}
path.save <- "C:/Users/jessi/University of Massachusetts/Jessica-BearProject - General/Final Data/Bear Information"
setwd(path.save)
write_xlsx(data, "C:/Users/jessi/University of Massachusetts/Jessica-BearProject - General/Final Data/Bear Information/Percent_Developed.xlsx")
```


### Percent Area Covered by Building Structures 

I calculated the percent area of building structures within a buffer of 100 meters around each point. The size of the buffer was determined by using the average step length for the 45-minute interval as the diameter for the buffer. The centroid of each of the building structures were extracted and then spatially joined to the buffered points. 

**Libraries**
Make sure to install and pull in the libraries you need in order for your code functions to work.
```{r}
library(rgdal)
library(raster)
library(sf)
library(adehabitatHR)
library(adehabitatHS)
library(reshape2)
library(dplyr)
```

**Set Paths**
Set the path pick the file folder to pull in your data \* It will be different based on where your data is stored
```{r}
path.root <- "C:/Users/jessi/University of Massachusetts/Jessica-BearProject - General/Final Data"
path.data <- paste(path.root, "/Combined Data/Combined, Clipped, Near", sep = "")
path.mcp <- paste(path.root, "/MCPs", sep = "")
path.GIS <- paste(path.root, "/GIS Layers", sep = "")
path.save <- paste(path.root, "/Combined Data/Combined, Clipped, Near, Percent Str", sep = "")
```

**Bring in the building structures layer**
```{r}
# Read building structures polygon shapefile
setwd(path.GIS)
bld_str <- st_read("building_structures.shp") 

# Check Class and CRS
class(bld_str)
crs(bld_str)
```

**Find the centroid of each building polygon**
```{r}
# Extract centroids and convert to points
bld_str_points <- st_centroid(bld_str)
```

**Bring In Use and Available Point Data**
```{r}
setwd(path.data)
# Use Points
use_data <- read_sf("Use_Points.shp")
# Available Points
avail_data <- read_sf("Available_Points.shp")
```

**Create buffers for use and availability each point**
Make sure there are the same amount of buffers as there are points. Add a FID to each buffer. 
```{r}
# Buffer the use location point by 100 meters
use_buffers <- st_buffer(use_data, dist = 100)
# Check to make sure there are the same amount of buffers as points
nrow(use_data)
nrow(use_buffers)
# Add buffer FID 
use_buffers$Buffer_FID <- 1:nrow(use_buffers)
print(use_buffers$Buffer_FID)

# Buffer the available location point by 100 meters
avail_buffers <- st_buffer(avail_data, dist = 100)
# Check to make sure there are the same amount of buffers as points
nrow(avail_data)
nrow(avail_buffers)
# Add Buffer FID
avail_buffers$Buffer_FID <- 1:nrow(avail_buffers)
```

**Join the use data and the building structure centroid points**
The area for the building structures is still attached to these points as SHAPE_Area.
```{r}
# Perform a spatial join between `use_buffers` and `bld_str_points`
joined_use <- st_join(use_buffers, bld_str_points)
head(joined_use)
```

**Turn the NAs to 0 for SHAPE_Area. This will allow us to summarize area of building structures in each buffer even if there is no building in the buffer.**
```{r}
joined_use$SHAPE_Area[is.na(joined_use$SHAPE_Area)] <- 0
```

**Sum the area if the building structures by the Buffer ID. This will add up all of the building structures area within each buffer.** 
```{r}
summary_use <- aggregate(SHAPE_Area ~ Buffer_FID, data = joined_use, FUN = sum)
head(summary_use)
```

**Calculate the percentage of building structure in each buffer**
Divide the building structures area by the area of the buffer (the area of a circle with a 100 m radius is 31415.93) then multiply by 100
```{r}
summary_use$Percent_Bld_Str <- ((summary_use$SHAPE_Area/31415.93)*100)
```

**Add the percentage of building structure to the Use data**
```{r}
use_data <- cbind(use_data, summary_use$Percent_Bld_Str)
colnames(use_data)[which(names(use_data) == "summary_use.Percent_Bld_Str")] <-"Percent_Bld_Str"
```

**Export as a Shapefile**
```{r}
setwd(path.save)
st_write(use_data, "Use_Points", driver = "ESRI Shapefile", append=FALSE)
```

**Repeat for available points data**
Join the available data and the building structure centroid points. The area for the building structures is still attached to these points as SHAPE_Area. 
```{r}
# Perform a spatial join between `use_buffers` and `bld_str_points`
joined_avail <- st_join(avail_buffers, bld_str_points)
head(joined_avail)
```

**Turn the NAs to 0 for SHAPE_Area**
This will allow us to summarize area of building structures in each buffer even if there is no building in the buffer.
```{r}
joined_avail$SHAPE_Area[is.na(joined_avail$SHAPE_Area)] <- 0
```

**Sum the area if the building structures by the Buffer ID**
This will add up all of the building structures area within each buffer.
```{r}
summary_avail <- aggregate(SHAPE_Area ~ Buffer_FID, data = joined_avail, FUN = sum)
head(summary_avail)
```

**Calculate the percentage of building structure in each buffer**
Divide the building structures area by the area of the buffer (the area of a circle with a 100 m radius is 31415.93) then multiply by 100
```{r}
summary_avail$Percent_Bld_Str <- ((summary_avail$SHAPE_Area/31415.93)*100)
```

**Add the percentage of building structure to the avail data**
```{r}
avail_data <- cbind(avail_data, summary_avail$Percent_Bld_Str)
colnames(avail_data)
colnames(avail_data)[which(names(avail_data) == "summary_avail.Percent_Bld_Str")] <-"Percent_Bld_Str"
```

**Export as a Shapefile**
```{r}
setwd(path.save)
st_write(avail_data, "Avail_Points", driver = "ESRI Shapefile", append=FALSE)
```

### Land Use




**Set Paths**
Set the path pick the file folder to pull in your data. It will be different based on where your data is stored
```{r}
path.root <- "C:/Users/jessi/University of Massachusetts/Jessica-BearProject - General/Final Data"
path.use.data <- paste(path.root, "/Combined Data/Combined, Clipped, Near, Percent Str/Use_Points", sep = "")
path.avail.data <- paste(path.root, "/Combined Data/Combined, Clipped, Near, Percent Str/Avail_Points", sep = "")
path.GIS <- paste(path.root, "/GIS Layers", sep = "")
path.save <- paste(path.root, "/Final Data", sep = "")
```



**Libraries**
Make sure to install and pull in the libraries you need in order for your code functions to work
```{r}
library(readr)
library(raster)
library(rgdal)
library(sf)
library(dplyr)
library(writexl)
```

**Pull in the Use Data shapefile and available data shapefile**
```{r}
# Use Data
setwd(path.use.data)
use_points <- st_read("Use_Points.shp")
# Available Data
setwd(path.avail.data)
avail_points <- st_read("Avail_Points.shp")
```

**Bring in the land cover 2016 data**
```{r}
# Set Path
setwd(path.GIS)
lulc2016 <-raster("LULC2016.tif")
```

**Examine the raster data**
```{r}
# Check the coordinate reference system
print(crs(lulc2016))

# Define the projection string for UTM Zone 18
UTM18 <- CRS("+init=epsg:32618")

# Define land use coordinate system
crs(lulc2016) <- UTM18
print(crs(lulc2016))
```

**Bring in file that cintains new land cover variables and classifications**
```{r}
setwd(path.GIS)
# This file has the new land cover variable and description
classification_cover <- read.csv("Land_Cover_Reclass.csv")
head(classification_cover)
```

**Reclassify map into fewer land cover categories**
```{r}
class <- as.matrix(classification_cover[,c(1,3)])
land_cover <- reclassify(lulc2016, rcl = class)
```

**Buffer the quabbin and get rid of available points in the quabbin to run again and see if there is a major difference in selection**
```{r}
# Read in quabbin shapefile
setwd(path.GIS)
quabbin <- read_sf("quabbin.shp")
crs(quabbin)


#Buffer by -50 meters, this goes 50 meter into the quabbin from the shore
# Add a -50 meter buffer
buffer <- st_buffer(quabbin, dist = -50)


# Change back to spatial data frame
buffer <- st_as_sf(buffer)

# Assign IDs to each point
avail_points$ID <- 1:nrow(avail_points)
 

# Select all of the points inside the quabbin, we will get rid of these
remove_points <- st_join(buffer, avail_points)
nrow(remove_points)

# Remove the points inside the quabbin and make sure the numbers are adding up correctly to ensure we aren't making a major mistake
# Remove points that were joined with the polygon
filtered_points <- avail_points[!avail_points$ID %in% remove_points$ID, ]
nrow(filtered_points)

# Check
nrow(avail_points)
nrow(filtered_points)+nrow(remove_points)
```

**Extract Land Cover Data to Points**
Remember the use_data already has all of the bear information, distance to nearest structure, and percent building structure for each point.
```{r}
use <- extract(land_cover, use_points)
avail <- extract(land_cover, filtered_points)
```

**Convert matrices into dataframes**
```{r}
# Convert the matrix or array into a data frame
use <- as.data.frame(use)
head(use)

avail <- as.data.frame(avail)
head(avail)
```

**Add Extracted Data to the Use and Available Data Dataframes**
This extracted data now has land cover code, distance to nearest structure, and percent building structure. Change the column names to be more clear.
```{r}
# Combine
use <- data.frame(cbind(use_points, use))
head(use)
# Change column names
names(use)[names(use) == 'Prc_B_S'] <- 'perc_bld_str'
names(use)[names(use) == 'NEAR_DI'] <- 'dis2str'
names(use)[names(use) == 'use'] <- 'landcover'
colnames(use)

# Combine
avail <- data.frame(cbind(filtered_points, avail))
head(avail)
# Change Column names
names(avail)[names(avail) == 'Prc_B_S'] <- 'perc_bld_str'
names(avail)[names(avail) == 'NEAR_DI'] <- 'dis2str'
names(avail)[names(avail) == 'avail'] <- 'landcover'
colnames(avail)
```

**Check to see how many and what points have no land cover data in use and available**
```{r}
# Use
use_before <- nrow(use)
# Remove NAs
use <- use[!is.na(use$landcover),]
# Number of rows lost to NA
use_before-nrow(use)

# Available
avail_before <- nrow(avail)
# Remove NAs
avail <- avail[!is.na(avail$landcover),]
# Number of rows lost to NA
avail_before-nrow(avail)
```

**Change the reproductive status "unknown" rows for broadbrook2014 to "none"**
```{r}
# Use
# Conditions
condition1 <- use$bear == "Broadbrook2014"
condition2 <- use$cb_stts == "unknown"

# Changing row value based on conditions
use$cb_stts[condition1 & condition2] <- "none"

# Available
# Conditions
condition1 <- avail$bear == "Broadbrook2014"
condition2 <- avail$cb_stts == "unknown"

# Changing row value based on conditions
avail$cb_stts[condition1 & condition2] <- "none"
```

**Change the reproductive status "unknown" rows for May2013 to "none"**
```{r}
# Use
# Conditions
condition1 <- use$bear == "May2013"
condition2 <- use$cb_stts == "unknown"

# Changing row value based on conditions
use$cb_stts[condition1 & condition2] <- "none"

# Available
# Conditions
condition1 <- avail$bear == "May2013"
condition2 <- avail$cb_stts == "unknown"

# Changing row value based on conditions
avail$cb_stts[condition1 & condition2] <- "none"
```

**Examine count of use points by seasons**
```{r}
seasons_sum <- use %>%
  group_by(season) %>%
  summarise(count = n())
head(seasons_sum)
```

**Remove NAs in data**
There were no NAs but better to do this step anyways
```{r}
# Remove NAs
use <- use[!is.na(use$season),]
```

**Save**
```{r}
# Set the path to where you want your outputs saved
setwd(path.save)

# Save Use Points Shapefile
st_write(use, "Use_Points", driver = "ESRI Shapefile", append=FALSE)

# Save Available Points Shapefile
st_write(avail, "Available_Points", driver = "ESRI Shapefile", append=FALSE)
```

## Models {.tabset}

### Correlation, Multicollinearity, and AICs

**Libraries**
Make sure to install and pull in the libraries you need in order for your code functions to work
```{r}
library(terra)
library(raster)
library(sf)
library(adehabitatHR)
library(adehabitatHS)
library(reshape2)
library(dplyr)
library(writexl)
library(readxl)
```

**Set Paths**
Set the path pick the file folder to pull in your data. It will be different based on where your data is stored
```{r}
path.root <- "C:/Users/jessi/University of Massachusetts/Jessica-BearProject - General/Final Data"
path.use.data <- paste(path.root, "/Final Data/Use_Points", sep = "")
path.avail.data <- paste(path.root, "/Final Data/Available_Points", sep = "")
path.gradient <- paste(path.root, "/Bear Information", sep = "")
```

**Bring In Use and Available Point Data**
This data contains the bear information and distance to nearest structure
```{r}
# Use Points
setwd(path.use.data)
use_data <- read_sf("Use_Points.shp")
# Available Points
setwd(path.avail.data)
avail_data <- read_sf("Available_Points.shp")
```

**Bring in the information that is the percent developed in the home range**
The value we are using for urbanization
```{r}
setwd(path.gradient)
gradient <- read_excel("Percent_Developed.xlsx")
gradient <- gradient[ ,1:2]
colnames(gradient) <- c("bear", "HR_perc_dev")
```

**Add the urbanization value to the points data sets**
```{r}
use_data <- merge(use_data, gradient, by = "bear", all = TRUE)
avail_data <- merge(avail_data, gradient, by = "bear", all = TRUE)
```

**Convert the reproductive status of "yearling" to "none"**
This will give us 2 options for reproductive status "none" and "newborn"
```{r}
# Change row values from "yearling" to "none"
use_data$cb_stts <- ifelse(use_data$cb_stts == "yearling", "none", use_data$cb_stts)
avail_data$cb_stts <- ifelse(avail_data$cb_stts == "yearling", "none", avail_data$cb_stts)
```

**Check the column names to see what data we have**
Note that the available data does not have season.
```{r}
colnames(use_data) 
colnames(avail_data)
```

**Create a dataframe to run in model**
This final data frame will be combined use and available points. The bear number will be used as the identifier for each point. The covariates will include land cover, nearest distance to structure, percent building structure, and use/availability as a binomial (use = 1, available = 0)
```{r}
# Subset the data by bear number and covariates
# br_nmbr is the bear number / bear ID
# dis2str is the distance to nearest structure
# landcvr is the reclassified lad cover
# prc_bl_ is the percent building structure in the buffered point
use <- subset(use_data, select = c(br_nmbr, year, cb_stts, season, landcvr, dis2str, prc_bl_, HR_perc_dev))
avail <- subset(avail_data, select = c(br_nmbr, year, cb_stts, landcvr, dis2str, prc_bl_, HR_perc_dev))
```

**Assign random seasons to availability points**
This is necessary when running a glm because all columns must be the same between use and available points
```{r}
avail$season <- rep(c("Summer", "Fall", "Spring"), length.out = nrow(avail))
# Change the order of columns to match use.2
avail <- avail[, c("br_nmbr", "year", "cb_stts", "season", "landcvr", "dis2str", "prc_bl_", "HR_perc_dev")]
```

**Combine the use and available data into one data frame**
```{r}
# Create data frame
# This adds use as a binomial
use.cov <- data.frame(use[,1:8], use = 1)
back.cov <- data.frame(avail[,1:8], use = 0)
# Combine the use and available points data to create one large dataframe
all.cov <- data.frame(rbind(use.cov, back.cov))
# Check the class of the landcover
class(all.cov$landcvr)
# Convert to factor
all.cov$landcvr <- as.factor(all.cov$landcvr)
class(all.cov$landcvr)
# Check the columns of the dataframe
colnames(all.cov)
```

**Correlation refers to the linear relationship between 2 variables**
Can be evaluated using	correlation coefficient.
A value of r close to -1: means that there is negative correlation between the variables (when one increases the other decreases and vice versa).
A value of r close to 0: indicates that the 2 variables are not correlated (no linear relationship exists between them).
A value of r close to 1: indicates a positive linear relationship between the 2 variables (when one increases, so does the other).
```{r}
# All varibables must be numeric to run cor()
all.cov$landcvr <- as.numeric(all.cov$landcvr)
class(all.cov$landcvr)
class(all.cov$dis2str)
class(all.cov$prc_bl_)

cor(all.cov[, 5:7])

# Based off these values, there is not significant correlation between any 2 variables, so all variables can be used in the model

all.cov$landcvr <- as.factor(all.cov$landcvr)
```

**Run a base model** to be able to compare model fit with different iterations of models with interactions. Use is the response variable, 0 is the intercept (used here to mean no selection). The fixed effects in the models included land cover, distance to nearest structure, and percent building structure area within a buffered point. Bear number is a random effect to account for variability of behavior among different bears. The family is binomial because the response variable (use) is either 1 or 0.

**Collinearity** refers to a problem when running a regression model where 2 or more independent variables (a.k.a. predictors) have a strong linear relationship. Can be evaluated using correlation matrix or VIF.
**Multicollinearity** is a special case of collinearity where a strong linear relationship exists between 3 or more independent variables even if no pair of variables has a high correlation. Can be evaluated using VIF.

VIF measures the strength of correlation between predictor variables in a model. It takes on a value between 1 and positive infinity.
VIF = 1: There is no correlation between a given predictor variable and any other predictor variables in the model.
VIF between 1 and 5: There is moderate correlation between a given predictor variable and other predictor variables in the model.
VIF > 5: There is severe correlation between a given predictor variable and other predictor variables in the model.
```{r}
library(car)

# define multiple linear regression model
model_all <- glm(use ~ landcvr + dis2str + prc_bl_, data = all.cov)

#calculate the VIF for each predictor variable in the model
vif(model_all)

# Based off these values, there is not multicollinearity, so all variables can be used in the model
```

**Check class of variables**
```{r}
class(all.cov$use)
class(all.cov$landcvr)
class(all.cov$dis2str)
class(all.cov$prc_bl_)
```

**Scale the continuous variables in order for the model to run more efficiently**
```{r}
# Step 1: Identify the columns to scale
columns_to_scale <- c("dis2str", "prc_bl_", "HR_perc_dev")

# Step 2: Subset the data frame
df_subset <- all.cov[, columns_to_scale]

# Step 3: Apply scale() to the subsetted data
scaled_data <- scale(df_subset, center = TRUE, scale = TRUE) 

# Step 4: Replace the original columns with scaled values
all.cov[, columns_to_scale] <- scaled_data
```

**Check all data to ensure that they are the correct classes and examine the data in the columns**
```{r}
head(all.cov)

# Column br_nmbr
class(all.cov$br_nmbr)
all.cov$br_nmbr <- as.factor(all.cov$br_nmbr)
levels(all.cov$br_nmbr)

# Column year
class(all.cov$year)
all.cov$year <- as.factor(all.cov$year)
levels(all.cov$year)

# Column cb_stts
class(all.cov$cb_stts)
all.cov$cb_stts <- as.factor(all.cov$cb_stts)
levels(all.cov$cb_stts)

# Column season
class(all.cov$season)
all.cov$season <- as.factor(all.cov$season)
levels(all.cov$season)

# Column landcvr
class(all.cov$landcvr)
levels(all.cov$landcvr)

# Column dis2str
class(all.cov$dis2str)

# Column prc_bl_
class(all.cov$prc_bl_)

# Column HR_perc_dev
class(all.cov$HR_perc_dev)

# Column use
class(all.cov$use)


# Remove the 'geometry' column from the data frame
all.cov <- all.cov[, -which(names(all.cov) == "geometry")]
head(all.cov)
```

**Run a base model** to be able to compare model fit with different iterations of models with interactions. Use is the response variable, 0 is the intercept (used here to mean no selection). The fixed effects in the models included land cover, distance to nearest structure, and percent building structure area within a buffered point. Bear number is a random effect to account for variability of behavior among different bears. The family is binomial because the response variable (use) is either 1 or 0.

```{r}
library(lme4)
# Create a glmerControl object with increased tolerance
control <- glmerControl(optimizer = "Nelder_Mead", optCtrl = list(tol = 0.005))
```


**Run multiple models with different interactions between fixed effects and reproductive status (criterion variable).**
```{r}
library(lme4)
# GLM with reproductive status
model_1.1 <- glmer(use ~ 0 + landcvr * cb_stts + (1 | br_nmbr), family = binomial, data = all.cov, control = control)
model_1.2 <- glmer(use ~ 0 + landcvr * cb_stts + dis2str * cb_stts + (1 | br_nmbr), family = binomial, data = all.cov, control = control)
model_1.3 <- glmer(use ~ 0 + landcvr * cb_stts + dis2str * cb_stts + prc_bl_ * cb_stts + (1 | br_nmbr), family = binomial, data = all.cov, control = control)
```

**Look at different Akaikeâ€™s Information Criterion (AIC) values** to see which models best fit the data.The model with the lowest AIC indicates the model best fit for the data. The delta AIC is a measure of fit of each model compared to the best fit model with a measurement of 0 indicating the model of best fit. In this case, model 1.3 is the best fit.
```{r}
library(AICcmodavg)

#define list of models
models_repro_status <- list(model_1.1, model_1.2, model_1.3)

#specify model names
mod.names.repro.status <- c("1.1", "1.2", "1.3")

#calculate AIC of each model
aictab(cand.set = models_repro_status, modnames = mod.names.repro.status)
```

**Save the model**
```{r}
setwd("C:/Users/jessi/University of Massachusetts/Jessica-BearProject - General/Final Data/Models")

# Save the model to an RDS file
saveRDS(model_1.3, file = "model_1.3.rds")
```

### Final Model


The RSF provides selection estimates that gave information on the change in selection for the predictor values. The selection estimates give the log-odds or selection change for every unit of change of the response variable. The odds ratio is used to compare the odds of habitat attributes being selected based on different variables. An odds ratio close to zero indicates zero selection behavior with use being proportional to availability. If the odds ratio is negative, there is selection against a particular habitat attribute. If the odds ratio is positive, there is selection for a particular habitat attribute. 


**Set Paths**
Set the path pick the file folder to pull in your data. It will be different based on where your data is stored
```{r}
path.models <-"C:/Users/jessi/University of Massachusetts/Jessica-BearProject - General/Final Data/Models"
```

**Pull in the model**
```{r}
setwd(path.models)
# Load the model back from the RDS file
model_1 <- readRDS("model_1.3.rds")
```

**Summarize the model**
```{r}
model_1_summary <- summary(model_1)
print(model_1_summary)
```
